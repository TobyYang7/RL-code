{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--device DEVICE] [--data DATA]\n",
      "                             [--train_val_ratio TRAIN_VAL_RATIO [TRAIN_VAL_RATIO ...]]\n",
      "                             [--h_layers H_LAYERS] [--in_channels IN_CHANNELS]\n",
      "                             [--hidden_channels HIDDEN_CHANNELS [HIDDEN_CHANNELS ...]]\n",
      "                             [--out_channels OUT_CHANNELS]\n",
      "                             [--emb_size EMB_SIZE] [--dropout DROPOUT]\n",
      "                             [--wemb_size WEMB_SIZE] [--time_d TIME_D]\n",
      "                             [--heads HEADS] [--support_len SUPPORT_LEN]\n",
      "                             [--order ORDER] [--num_weather NUM_WEATHER]\n",
      "                             [--use_se USE_SE] [--use_cov USE_COV]\n",
      "                             [--decay DECAY] [--lr LR] [--in_len IN_LEN]\n",
      "                             [--out_len OUT_LEN] [--batch BATCH]\n",
      "                             [--episode EPISODE] [--period PERIOD]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"83731afe-ee11-4764-b71f-5fdcbde1eae6\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=/Users/yuzheyang/Library/Jupyter/runtime/kernel-v2-97037GbE427vrnWUR.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--device', type=str, default='cuda:0', help='')\n",
    "parser.add_argument('--data', type=str, default='US', help='data type')\n",
    "parser.add_argument(\"--train_val_ratio\", nargs=\"+\",\n",
    "                    default=[0.7, 0.1], help='train/val/test ratio', type=float)\n",
    "parser.add_argument('--h_layers', type=int, default=2,\n",
    "                    help='number of hidden layer')\n",
    "parser.add_argument('--in_channels', type=int,\n",
    "                    default=2, help='input variable')\n",
    "parser.add_argument(\"--hidden_channels\", nargs=\"+\",\n",
    "                    default=[128, 64, 32], help='hidden layer dimension', type=int)\n",
    "parser.add_argument('--out_channels', type=int,\n",
    "                    default=2, help='output variable')\n",
    "parser.add_argument('--emb_size', type=int, default=16,\n",
    "                    help='time embedding size')\n",
    "parser.add_argument('--dropout', type=float, default=0, help='dropout rate')\n",
    "parser.add_argument('--wemb_size', type=int, default=4,\n",
    "                    help='covairate embedding size')\n",
    "parser.add_argument('--time_d', type=int, default=4,\n",
    "                    help='normalizing factor for self-attention model')\n",
    "parser.add_argument('--heads', type=int, default=4,\n",
    "                    help='number of attention heads')\n",
    "parser.add_argument('--support_len', type=int, default=3,\n",
    "                    help='number of spatial adjacency matrix')\n",
    "parser.add_argument('--order', type=int, default=2,\n",
    "                    help='order of diffusion convolution')\n",
    "parser.add_argument('--num_weather', type=int, default=8,\n",
    "                    help='number of weather condition')\n",
    "parser.add_argument('--use_se', type=str, default=True, help=\"use SE block\")\n",
    "parser.add_argument('--use_cov', type=str, default=True, help=\"use Covariate\")\n",
    "parser.add_argument('--decay', type=float, default=1e-5,\n",
    "                    help='decay rate of learning rate ')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate ')\n",
    "parser.add_argument('--in_len', type=int, default=12,\n",
    "                    help='input time series length')\n",
    "parser.add_argument('--out_len', type=int, default=12,\n",
    "                    help='output time series length')\n",
    "parser.add_argument('--batch', type=int, default=32,\n",
    "                    help='training batch size')\n",
    "parser.add_argument('--episode', type=int, default=50,\n",
    "                    help='training episodes')\n",
    "parser.add_argument('--period', type=int, default=36,\n",
    "                    help='periodic for temporal embedding')\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(args.device)\n",
    "    adj, training_data, val_data, test_data, training_w, val_w, test_w = util.load_data(\n",
    "        args.data)\n",
    "    # todo\n",
    "    model = STPN(args.h_layers, args.in_channels, args.hidden_channels, args.out_channels, args.emb_size,\n",
    "                 args.dropout, args.wemb_size, args.time_d, args.heads, args.support_len,\n",
    "                 args.order, args.num_weather, args.use_se, args.use_cov).to(device)\n",
    "    supports = [torch.tensor(i).to(device) for i in adj]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr,\n",
    "                           weight_decay=args.decay)\n",
    "    scaler = StandardScaler(training_data[~np.isnan(training_data)].mean(\n",
    "    ), training_data[~np.isnan(training_data)].std())\n",
    "    training_data = scaler.transform(training_data)\n",
    "    training_data[np.isnan(training_data)] = 0\n",
    "\n",
    "    MAE_list = []\n",
    "    batch_index = list(\n",
    "        range(training_data.shape[1] - (args.in_len + args.out_len)))\n",
    "    val_index = list(range(val_data.shape[1] - (args.in_len + args.out_len)))\n",
    "    label = []\n",
    "    for i in range(len(val_index)):\n",
    "        label.append(np.expand_dims(\n",
    "            val_data[:, val_index[i] + args.in_len:val_index[i] + args.in_len + args.out_len, :], axis=0))\n",
    "    label = np.concatenate(label)\n",
    "\n",
    "    print(\"start training...\", flush=True)\n",
    "\n",
    "    for ep in range(1, 1+args.episode):\n",
    "        random.shuffle(batch_index)\n",
    "        for j in range(len(batch_index) // args.batch - 1):\n",
    "            trainx = []\n",
    "            trainy = []\n",
    "            trainti = []\n",
    "            trainto = []\n",
    "            trainw = []\n",
    "            for k in range(args.batch):\n",
    "                trainx.append(np.expand_dims(\n",
    "                    training_data[:, batch_index[j * args.batch + k]: batch_index[j * args.batch + k] + args.in_len, :], axis=0))\n",
    "                trainy.append(np.expand_dims(training_data[:, batch_index[j * args.batch + k] +\n",
    "                              args.in_len:batch_index[j * args.batch + k] + args.in_len + args.out_len, :], axis=0))\n",
    "                trainw.append(np.expand_dims(\n",
    "                    training_w[:, batch_index[j * args.batch + k]: batch_index[j * args.batch + k] + args.in_len], axis=0))\n",
    "                trainti.append((np.arange(batch_index[j * args.batch + k], batch_index[j * args.batch +\n",
    "                               k] + args.in_len) % args.period) * np.ones([1, args.in_len])/(args.period - 1))\n",
    "                trainto.append((np.arange(batch_index[j * args.batch + k] + args.in_len, batch_index[j * args.batch + k] +\n",
    "                               args.in_len + args.out_len) % args.period) * np.ones([1, args.out_len])/(args.period - 1))\n",
    "            trainx = np.concatenate(trainx)\n",
    "            trainti = np.concatenate(trainti)\n",
    "            trainto = np.concatenate(trainto)\n",
    "            trainy = np.concatenate(trainy)\n",
    "            trainw = np.concatenate(trainw)\n",
    "            trainw = torch.LongTensor(trainw).to(device)\n",
    "            trainx = torch.Tensor(trainx).to(device)\n",
    "            trainx = trainx.permute(0, 3, 1, 2)\n",
    "            trainy = torch.Tensor(trainy).to(device)\n",
    "            trainy = trainy.permute(0, 3, 1, 2)\n",
    "            trainti = torch.Tensor(trainti).to(device)\n",
    "            trainto = torch.Tensor(trainto).to(device)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(trainx, trainti, supports, trainto, trainw)\n",
    "            loss = util.masked_rmse(output, trainy, 0.0)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "\n",
    "        outputs = []\n",
    "        model.eval()\n",
    "        for i in range(len(val_index)):\n",
    "            testx = np.expand_dims(\n",
    "                val_data[:, val_index[i]: val_index[i] + args.in_len, :], axis=0)\n",
    "            testx = scaler.transform(testx)\n",
    "            testw = np.expand_dims(\n",
    "                val_w[:, val_index[i]: val_index[i] + args.in_len], axis=0)\n",
    "            testw = torch.LongTensor(testw).to(device)\n",
    "            testx[np.isnan(testx)] = 0\n",
    "            testti = (np.arange(int(training_data.shape[1])+val_index[i], int(\n",
    "                training_data.shape[1])+val_index[i] + args.in_len) % args.period) * np.ones([1, args.in_len])/(args.period - 1)\n",
    "            testto = (np.arange(int(training_data.shape[1])+val_index[i] + args.in_len, int(training_data.shape[1]) +\n",
    "                      val_index[i] + args.in_len + args.out_len) % args.period) * np.ones([1, args.out_len])/(args.period - 1)\n",
    "            testx = torch.Tensor(testx).to(device)\n",
    "            testx = testx.permute(0, 3, 1, 2)\n",
    "            testti = torch.Tensor(testti).to(device)\n",
    "            testto = torch.Tensor(testto).to(device)\n",
    "            output = model(testx, testti, supports, testto, testw)\n",
    "            output = output.permute(0, 2, 3, 1)\n",
    "            output = output.detach().cpu().numpy()\n",
    "            output = scaler.inverse_transform(output)\n",
    "            outputs.append(output)\n",
    "        yhat = np.concatenate(outputs)\n",
    "\n",
    "        amae = []\n",
    "        ar2 = []\n",
    "        armse = []\n",
    "        for i in range(12):\n",
    "            metrics = test_error(yhat[:, :, i, :], label[:, :, i, :])\n",
    "            amae.append(metrics[0])\n",
    "            ar2.append(metrics[2])\n",
    "            armse.append(metrics[1])\n",
    "\n",
    "        log = 'On average over all horizons, Test MAE: {:.4f}, Test R2: {:.4f}, Test RMSE: {:.4f}'\n",
    "        print(log.format(np.mean(amae), np.mean(ar2), np.mean(armse)))\n",
    "\n",
    "        MAE_list.append(np.mean(amae))\n",
    "        if np.mean(amae) == min(MAE_list):\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model)\n",
    "    torch.save(model, \"spdpn\" + args.data + \".pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
